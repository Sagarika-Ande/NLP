{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIURPSd4p91IwySXJx/vIW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sagarika-Ande/NLP/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMfam35azIPs",
        "outputId": "7b6fae3a-0890-4ef6-f1a3-11c5c536b61d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlaRGvht0yc9",
        "outputId": "c67c3192-6af4-4561-fb8b-76ed3dddfc06"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZATION WITH NLTK\n"
      ],
      "metadata": {
        "id": "SmisVc_N3aMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=\"\"\"Hello welcome, to Krish Naik's NLP Tutorials.\n",
        "Please do watch the entire course! to become expert in NLP\"\"\"\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3hSBhwTzMLZ",
        "outputId": "62ccb08c-9275-47e6-d7bc-aae2caf0a19d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello welcome, to Krish Naik's NLP Tutorials.\n",
            "Please do watch the entire course! to become expert in NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bKP_RJvzMHv",
        "outputId": "84d64bea-c170-46f0-85f4-157416eebfc0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Hello welcome, to Krish Naik's NLP Tutorials.\",\n",
              " 'Please do watch the entire course!',\n",
              " 'to become expert in NLP']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4YKbj63zMD4",
        "outputId": "3e25dcd9-a6f7-4562-d37f-bfa2c308f0b4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'Krish',\n",
              " 'Naik',\n",
              " \"'s\",\n",
              " 'NLP',\n",
              " 'Tutorials',\n",
              " '.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEMMING:reducing a word to its word stem\n",
        "[eaten,eating,eats][go,going,gone]\n",
        "This Two are not perfectly stemming the words thats why lemataization come into picture"
      ],
      "metadata": {
        "id": "IFAqnAZE3ied"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from nltk.stem import RegexpStemmer\n",
        " reg_stem=RegexpStemmer('ing|ed')\n",
        " reg_stem.stem('ingeating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UlvSLbDhzMBv",
        "outputId": "ef242083-7ca3-4620-ae3b-e38c84b0fd8e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer # Import the SnowballStemmer class\n",
        "\n",
        "snowballststemmer = SnowballStemmer(\"english\")  # Create an instance of the SnowballStemmer for English\n",
        "snowballststemmer.stem('goes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bmxVEtgh9NQi",
        "outputId": "4c50d6e0-2dee-4bcb-b0d7-f4313615ecc0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEMMATIZATION:its like stemming the output will lemma. root word rather then root stem"
      ],
      "metadata": {
        "id": "SFrHzUPp9Ukx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbhBZS3YB9AL",
        "outputId": "6fd68d12-a339-43d9-df8f-f28ad2dc7426"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('going')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IdS6S5pazL_G",
        "outputId": "081cca11-0bad-4a06-a16d-5a0d8e16fbd7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'going'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('goes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ed0CgcH3zL8g",
        "outputId": "a477e855-b252-4fee-97f3-e5ae956e1b50"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STOPWORDS:"
      ],
      "metadata": {
        "id": "E-ajsu_bDORf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig2jobyAzL5q",
        "outputId": "ce15d8db-ec85-4039-d530-24952299f28b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"The world needs your brilliance, your unique perspective, and your unwavering spirit. You possess the strength to navigate challenges and achieve your goals. Remember, success isn't just about the destination, it's about the journey and the lessons learned along the way. Believe in your potential, embrace challenges, and never give up on your dreams. You are capable of greatness, and it's time to unleash your full potential!\"\n"
      ],
      "metadata": {
        "id": "4J6-teawzLuv"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIptUiAiK0fg",
        "outputId": "246c8054-8b8c-4640-9fec-2457f7e6f32b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()\n",
        "sentences=nltk.sent_tokenize(paragraph)\n",
        "type(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIJ4ngvC3l82",
        "outputId": "5b641c19-1855-47ef-85b7-849ed59d4db5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stopwords and filter and then apply stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  wordlist=nltk.word_tokenize(sentences[i])\n",
        "  wordlist=[stemmer.stem(word) for word in wordlist if word not in set(stopwords.words('english'))]\n",
        "  sentences[i]=' '.join(wordlist)#converting all the words into sentences"
      ],
      "metadata": {
        "id": "ESwPjWf63l5U"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg3eLMQF3l3L",
        "outputId": "db51b732-edb6-4b53-a9f1-9114f9029312"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['world need brillianc , uniqu perspect , unwav spirit .',\n",
              " 'possess strength navig challeng achiev goal .',\n",
              " \"rememb , success n't destin , 's journey lesson learn along way .\",\n",
              " 'believ potenti , embrac challeng , never give dream .',\n",
              " \"capabl great , 's time unleash full potenti !\"]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPEECH TAGGING:parts of speech tagging\n",
        "\n"
      ],
      "metadata": {
        "id": "CS_u_9RRLcqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg6EGtOUPFdp",
        "outputId": "f50d3952-617d-4468-86ff-c852f6396609"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will find the pos tag\n",
        "for i in range(len(sentences)):\n",
        "  wordlist=nltk.word_tokenize(sentences[i])\n",
        "  wordlist=[word for word in wordlist if word not in set(stopwords.words('english'))]\n",
        "  pos_tag=nltk.pos_tag(wordlist) # Explicitly specify the language as 'eng'\n",
        "  print(pos_tag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rrLpJZk3l0m",
        "outputId": "c17bcb11-3418-4382-e426-2ebd1e4a4283"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('world', 'NN'), ('need', 'NN'), ('brillianc', 'NN'), (',', ','), ('uniqu', 'JJ'), ('perspect', 'NN'), (',', ','), ('unwav', 'JJ'), ('spirit', 'NN'), ('.', '.')]\n",
            "[('possess', 'NN'), ('strength', 'NN'), ('navig', 'JJ'), ('challeng', 'NN'), ('achiev', 'NN'), ('goal', 'NN'), ('.', '.')]\n",
            "[('rememb', 'NN'), (',', ','), ('success', 'NN'), (\"n't\", 'RB'), ('destin', 'VBD'), (',', ','), (\"'s\", 'POS'), ('journey', 'NN'), ('lesson', 'NN'), ('learn', 'VBP'), ('along', 'IN'), ('way', 'NN'), ('.', '.')]\n",
            "[('believ', 'NN'), ('potenti', 'NN'), (',', ','), ('embrac', 'FW'), ('challeng', 'NN'), (',', ','), ('never', 'RB'), ('give', 'VBP'), ('dream', 'NN'), ('.', '.')]\n",
            "[('capabl', 'NN'), ('great', 'JJ'), (',', ','), (\"'s\", 'POS'), ('time', 'NN'), ('unleash', 'JJ'), ('full', 'JJ'), ('potenti', 'NN'), ('!', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NAMED ENTITY RECOGNITION"
      ],
      "metadata": {
        "id": "sgDvM8V2Qxrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"We do not need magic to change the world, we carry all the power we need inside ourselves already: we have the power to imagine better\""
      ],
      "metadata": {
        "id": "Y16cl1Ls3lx8"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "words=nltk.word_tokenize(sentence)"
      ],
      "metadata": {
        "id": "zDmnBI_CIzwn"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_elements=nltk.pos_tag(words)\n",
        "tag_elements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU2YBBoHIztM",
        "outputId": "fe01c08e-5e31-4922-aee8-c33d1168bfed"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We', 'PRP'),\n",
              " ('do', 'VBP'),\n",
              " ('not', 'RB'),\n",
              " ('need', 'VB'),\n",
              " ('magic', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('change', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('world', 'NN'),\n",
              " (',', ','),\n",
              " ('we', 'PRP'),\n",
              " ('carry', 'VBP'),\n",
              " ('all', 'PDT'),\n",
              " ('the', 'DT'),\n",
              " ('power', 'NN'),\n",
              " ('we', 'PRP'),\n",
              " ('need', 'VBP'),\n",
              " ('inside', 'JJ'),\n",
              " ('ourselves', 'PRP'),\n",
              " ('already', 'RB'),\n",
              " (':', ':'),\n",
              " ('we', 'PRP'),\n",
              " ('have', 'VBP'),\n",
              " ('the', 'DT'),\n",
              " ('power', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('imagine', 'VB'),\n",
              " ('better', 'JJR')]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tH3xWacARmEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iKyLcTVyIzrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6iF636mIzo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YTHAGhwZIzmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p5je5KbiIzj2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}